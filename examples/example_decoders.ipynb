{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Core dependencies ────────────────────────────────────────────────\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ─── DeepSynergy modules ─────────────────────────────────────────────\n",
    "from deepsynergy import decoders\n",
    "from deepsynergy.utils_training import train_decoder\n",
    "\n",
    "# ─── Device configuration ────────────────────────────────────────────\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Sanity Check Notebook\n",
    "\n",
    "This notebook validates the correctness of individual decoder heads in DeepSynergy.\n",
    "\n",
    "Each section constructs a *synthetic* conditional distribution \\( p(x \\mid z) \\) with known entropy. A corresponding **decoder** is then trained to approximate this distribution using neural networks. If successful, the decoder's average cross-entropy will closely match the theoretical conditional entropy \\( H(X \\mid Z) \\).\n",
    "\n",
    "This serves both as a test of decoder implementation and as a diagnostic tool for debugging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 · BinaryDecoder — Binary Symmetric Channel\n",
    "\n",
    "We simulate a binary-symmetric channel (BSC) where:\n",
    "\n",
    "- $ Z \\in \\{0, 1\\} $ with $ \\mathbb{P}(Z = 1) = 0.5 $\n",
    "- The output $X$ is a noisy copy of $ Z $, flipped with probability $ \\varepsilon $\n",
    "\n",
    "This models the conditional distribution:\n",
    "$$\n",
    "p(x \\mid z) = \n",
    "\\begin{cases}\n",
    "1 - \\varepsilon & \\text{if } x = z \\\\\n",
    "\\varepsilon & \\text{if } x \\neq z\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The theoretical conditional entropy is:\n",
    "$$\n",
    "H(X \\mid Z) = -\\varepsilon \\log_2 \\varepsilon - (1 - \\varepsilon)\\log_2(1 - \\varepsilon)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:31<00:00, 10.97it/s, loss=[0.47147176]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(X|Z)  analytic : 0.469 bits\n",
      "H(X|Z)  decoder  : 0.471 bits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ─── Parameters ──────────────────────────────────────────────────────\n",
    "samples  = 10_000       # Number of (Z, X) pairs\n",
    "epsilon  = 0.10         # Bit-flip probability (channel noise)\n",
    "\n",
    "# ─── Theoretical entropy H(X|Z) for binary-symmetric channel ─────────\n",
    "H_theory = -epsilon * np.log2(epsilon) - (1 - epsilon) * np.log2(1 - epsilon)\n",
    "\n",
    "# ─── Synthetic dataset generation ────────────────────────────────────\n",
    "Z_np = np.random.randint(0, 2, size=(samples, 1))                     # Binary source\n",
    "noise = (np.random.rand(samples, 1) < epsilon).astype(np.int32)\n",
    "X_np = np.bitwise_xor(Z_np, noise).astype(np.float32)                 # Flip with prob ε\n",
    "\n",
    "# ─── Torch tensors + dataloader ─────────────────────────────────────\n",
    "Z = torch.FloatTensor(Z_np)\n",
    "X = torch.FloatTensor(X_np)\n",
    "dataloader = DataLoader(TensorDataset(Z, X), batch_size=samples)\n",
    "\n",
    "# ─── Decoder network setup ───────────────────────────────────────────\n",
    "decoder = decoders.BinaryDecoder(\n",
    "    nn.Sequential(\n",
    "        nn.Linear(1, 16), nn.ReLU(),\n",
    "        nn.Linear(16, 16), nn.ReLU(),\n",
    "        nn.Linear(16, 1)                    # One logit per binary variable\n",
    "    )\n",
    ").to(device)\n",
    "\n",
    "# ─── Train decoder to approximate p(x | z) ───────────────────────────\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)\n",
    "decoder_results = train_decoder(\n",
    "    model         = decoder,\n",
    "    dataloader    = dataloader,\n",
    "    optimizer     = optimizer,\n",
    "    show_progress = True,\n",
    "    device        = device,\n",
    "    epochs        = 1000\n",
    ")\n",
    "\n",
    "# ─── Compare analytical and empirical entropy ────────────────────────\n",
    "H_decoder = decoder_results['loss'][0]\n",
    "\n",
    "print(f\"H(X|Z)  analytic : {H_theory:.3f} bits\")\n",
    "print(f\"H(X|Z)  decoder  : {H_decoder:.3f} bits\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 · CategoricalDecoder — N-ary Symmetric Channel\n",
    "\n",
    "We simulate an N-ary symmetric channel:\n",
    "\n",
    "- $Z$ is uniformly distributed over $\\{0, 1, \\dots, N-1\\}$\n",
    "- With probability $\\varepsilon$, the output class $X$ is replaced with a random incorrect class\n",
    "\n",
    "This models a noisy multi-class classification problem with uniform corruption.\n",
    "\n",
    "The conditional entropy is:\n",
    "\n",
    "$$\n",
    "H(X \\mid Z) =\n",
    "-(1 - \\varepsilon) \\log_2(1 - \\varepsilon)\n",
    "- \\varepsilon \\log_2\\left(\\frac{\\varepsilon}{N - 1}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:33<00:00, 10.75it/s, loss=[1.1356778]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(X|Z)  analytic : 1.122 bits\n",
      "H(X|Z)  decoder  : 1.136 bits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ─── Parameters ──────────────────────────────────────────────────────\n",
    "samples = 10_000\n",
    "N = 5                  # Number of classes\n",
    "epsilon = 0.20         # Error probability\n",
    "\n",
    "# ─── Theoretical entropy H(X|Z) for N-ary symmetric channel ──────────\n",
    "H_theory = -(1 - epsilon) * np.log2(1 - epsilon) - epsilon * np.log2(epsilon / (N - 1))\n",
    "\n",
    "# ─── Synthetic dataset generation ────────────────────────────────────\n",
    "Z_np = np.random.randint(N, size=(samples, 1))     # True class\n",
    "X_np = Z_np.copy()                                 # Initially identical\n",
    "\n",
    "# Corrupt with probability epsilon\n",
    "flip = np.random.rand(samples, 1) < epsilon\n",
    "X_np[flip] = (Z_np[flip] + np.random.randint(1, N, size=flip.sum())) % N\n",
    "\n",
    "# ─── Torch tensors + dataloader ─────────────────────────────────────\n",
    "Z = torch.FloatTensor(Z_np)\n",
    "X = torch.FloatTensor(X_np)\n",
    "dataloader = DataLoader(TensorDataset(Z, X), batch_size=samples)\n",
    "\n",
    "# ─── Decoder network ────────────────────────────────────────────────\n",
    "decoder = decoders.CategoricalDecoder(\n",
    "    nn.Sequential(\n",
    "        nn.Linear(1, 8), nn.ReLU(),\n",
    "        nn.Linear(8, 16), nn.ReLU(),\n",
    "        nn.Linear(16, 8), nn.ReLU(),\n",
    "    ),\n",
    "    output_dim = 1,\n",
    "    num_classes = N,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr=3e-3)\n",
    "\n",
    "# ─── Training ───────────────────────────────────────────────────────\n",
    "decoder_results = train_decoder(\n",
    "    model         = decoder,\n",
    "    dataloader    = dataloader,\n",
    "    optimizer     = optimizer,\n",
    "    show_progress = True,\n",
    "    device        = device,\n",
    "    epochs        = 1000,\n",
    ")\n",
    "\n",
    "H_decoder = decoder_results['loss'][0]\n",
    "\n",
    "print(f\"H(X|Z)  analytic : {H_theory:.3f} bits\")\n",
    "print(f\"H(X|Z)  decoder  : {H_decoder:.3f} bits\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 · GaussianDecoder — Heteroscedastic Normal\n",
    "\n",
    "We simulate a conditional Gaussian where the output variance depends on the input:\n",
    "\n",
    "- $Z \\sim \\mathcal{N}(0, 1)$\n",
    "- $X \\mid Z = z \\sim \\mathcal{N}(0, z^2)$\n",
    "\n",
    "This is a heteroscedastic model where the noise increases with the magnitude of $Z$.\n",
    "\n",
    "The differential entropy is:\n",
    "\n",
    "$$\n",
    "H(X \\mid Z)\n",
    "= \\tfrac{1}{2} \\log_2(2\\pi e)\n",
    "- \\tfrac{1}{2} \\cdot \\frac{\\gamma + \\log 2}{\\log 2}\n",
    "\\approx 1.131 \\ \\text{bits}\n",
    "$$\n",
    "\n",
    "where $\\gamma \\approx 0.577$ is the Euler–Mascheroni constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuzzi/Projects/DeepSynergy/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "100%|██████████| 1000/1000 [01:23<00:00, 12.01it/s, loss=[1.167509]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(X|Z)  analytic : 1.131 bits\n",
      "H(X|Z)  decoder  : 1.168 bits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ─── Parameters ──────────────────────────────────────────────────────\n",
    "samples = 10_000\n",
    "\n",
    "# ─── Theoretical entropy H(X|Z) for N(0, Z^2) ────────────────────────\n",
    "H_theory = (\n",
    "    0.5 * np.log(2 * np.pi * np.e)           # ½ log(2πe)\n",
    "    - 0.5 * (np.log(2) + np.euler_gamma)     # −½(γ + log 2)\n",
    ") / np.log(2)\n",
    "\n",
    "# ─── Synthetic dataset generation ────────────────────────────────────\n",
    "Z_np = np.random.randn(samples, 1)\n",
    "X_np = np.random.randn(samples, 1) * np.abs(Z_np)    # σ = |Z|\n",
    "\n",
    "# ─── Torch tensors + dataloader ─────────────────────────────────────\n",
    "Z = torch.FloatTensor(Z_np)\n",
    "X = torch.FloatTensor(X_np)\n",
    "dataloader = DataLoader(TensorDataset(Z, X), batch_size=samples)\n",
    "\n",
    "# ─── Decoder network ────────────────────────────────────────────────\n",
    "decoder = decoders.GaussianDecoder(\n",
    "    nn.Sequential(\n",
    "        nn.Linear(1, 8), nn.ReLU(),\n",
    "        nn.Linear(8, 16), nn.ReLU(),\n",
    "        nn.Linear(16, 8), nn.ReLU(),\n",
    "    ),\n",
    "    output_dim = 1,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr=3e-3)\n",
    "\n",
    "# ─── Training ───────────────────────────────────────────────────────\n",
    "decoder_results = train_decoder(\n",
    "    model         = decoder,\n",
    "    dataloader    = dataloader,\n",
    "    optimizer     = optimizer,\n",
    "    show_progress = True,\n",
    "    device        = device,\n",
    "    epochs        = 1000,\n",
    ")\n",
    "\n",
    "H_decoder = decoder_results['loss'][0]\n",
    "\n",
    "print(f\"H(X|Z)  analytic : {H_theory:.3f} bits\")\n",
    "print(f\"H(X|Z)  decoder  : {H_decoder:.3f} bits\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 · GaussianMixtureDecoder — Laplace Scale from Z\n",
    "\n",
    "We simulate a conditional Laplace distribution where the scale depends on the input:\n",
    "\n",
    "- $Z \\sim \\mathrm{Exp}(1)$\n",
    "- $X \\mid Z = z \\sim \\mathrm{Laplace}(0, z)$\n",
    "\n",
    "This defines a heavy-tailed, heteroscedastic conditional distribution.\n",
    "\n",
    "The differential entropy of $X \\mid Z$ is:\n",
    "\n",
    "$$\n",
    "H(X \\mid Z) =\n",
    "\\frac{1 + \\log 2 - \\gamma}{\\log 2}\n",
    "\\approx 1.608\\ \\text{bits}\n",
    "$$\n",
    "\n",
    "where $\\gamma$ is the Euler–Mascheroni constant.  \n",
    "A Gaussian mixture with $K=5$ components should approximate the Laplace distribution sufficiently well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:23<00:00, 12.03it/s, loss=[1.672348]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(X|Z)  analytic : 1.610 bits\n",
      "H(X|Z)  decoder  : 1.672 bits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ─── Parameters ──────────────────────────────────────────────────────\n",
    "samples = 10_000\n",
    "K       = 5                  # Number of mixture components\n",
    "\n",
    "# ─── Theoretical H(X|Z) for Laplace(0, Z) ────────────────────────────\n",
    "H_theory = (1 + np.log(2) - np.euler_gamma) / np.log(2)\n",
    "\n",
    "# ─── Synthetic dataset generation ────────────────────────────────────\n",
    "Z_np = np.random.exponential(scale=1.0, size=(samples, 1))\n",
    "X_np = np.random.laplace(loc=0.0, scale=Z_np)        # Laplace(scale = Z)\n",
    "\n",
    "# ─── Torch tensors + dataloader ─────────────────────────────────────\n",
    "Z = torch.FloatTensor(Z_np)\n",
    "X = torch.FloatTensor(X_np)\n",
    "dataloader = DataLoader(TensorDataset(Z, X), batch_size=samples)\n",
    "\n",
    "# ─── Decoder network ────────────────────────────────────────────────\n",
    "decoder = decoders.GaussianMixtureDecoder(\n",
    "    nn.Sequential(\n",
    "        nn.Linear(1, 8),  nn.ReLU(),\n",
    "        nn.Linear(8, 16), nn.ReLU(),\n",
    "        nn.Linear(16, 8), nn.ReLU(),\n",
    "    ),\n",
    "    output_dim     = 1,\n",
    "    num_components = K,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)\n",
    "\n",
    "# ─── Training ───────────────────────────────────────────────────────\n",
    "decoder_results = train_decoder(\n",
    "    model         = decoder,\n",
    "    dataloader    = dataloader,\n",
    "    optimizer     = optimizer,\n",
    "    show_progress = True,\n",
    "    device        = device,\n",
    "    epochs        = 1000,\n",
    ")\n",
    "\n",
    "H_decoder = decoder_results['loss'][0]\n",
    "\n",
    "print(f\"H(X|Z)  analytic : {H_theory:.3f} bits\")\n",
    "print(f\"H(X|Z)  decoder  : {H_decoder:.3f} bits\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
